{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pickle as pkl\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据获取和处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据目录下保存了很多csv文件，每个csv文件保存了一个5s的sequence，包含了多个车辆的轨迹信息\n",
    "DATASET_PATH = \"/ssd/datasets/argoverse/argoverse-forecasting-dataset/\"\n",
    "TRAIN = DATASET_PATH + \"train/data\"\n",
    "VAL = DATASET_PATH + \"val/data\"\n",
    "TEST = DATASET_PATH + \"test_obs/data/\"\n",
    "\n",
    "def data_process(root_dir, mode):\n",
    "    \"\"\"\n",
    "    处理root_dir目录下的CSV文件\n",
    "    返回numpy array, shape: (total num of sequences, seq len, 4)\n",
    "    \"\"\"\n",
    "    root_dir = pathlib.Path(root_dir)\n",
    "    paths = [(root_dir / filename).absolute() for filename in os.listdir(root_dir)]\n",
    "    seq_len = 50 if mode != \"test\" else 20\n",
    "    features = np.empty((len(paths), seq_len, 4))\n",
    "    for i in tqdm((range(len(paths)))):\n",
    "        path = paths[i]\n",
    "        sequence = pd.read_csv(path)\n",
    "        agent_x = sequence[sequence[\"OBJECT_TYPE\"] == \"AGENT\"][\"X\"]\n",
    "        agent_y = sequence[sequence[\"OBJECT_TYPE\"] == \"AGENT\"][\"Y\"]\n",
    "        xy = np.column_stack((agent_x, agent_y))\n",
    "        # 如果是train或者val，则xy shape: (50, 2) 记录了5秒（每秒10帧）的agent xy坐标\n",
    "        # 否则xy shape: (20, 2)，是agent前2s的xy坐标\n",
    "        vel = xy[1:] - xy[:-1]\n",
    "        init_unknown_vel = np.array([np.nan, np.nan])\n",
    "        vel = np.vstack((init_unknown_vel, vel))\n",
    "        # vel shape: (seq len, 2)，差分得到速度，初始速度无法获取，设为NaN\n",
    "        feature = np.column_stack((xy, vel))\n",
    "        # feature shape: (seq len, 4), 各列分别是x, y, vel_x, vel_y\n",
    "        features[i] = feature\n",
    "    return features\n",
    "\n",
    "def save_features_to_pkl(features, filepath):\n",
    "    basedir = os.path.dirname(filepath)\n",
    "    if not os.path.exists(basedir):\n",
    "        os.makedirs(basedir)\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pkl.dump(features, f)\n",
    "        \n",
    "def load_pkl_to_features(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        features = pkl.load(f)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8155dadd4b854fd2b440fe77bbaa3d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d = {\"train\": TRAIN,\n",
    "     \"val\": VAL,\n",
    "     \"test\": TEST} \n",
    "for mode, path in tqdm(d.items()):\n",
    "    save_path = 'data/{}.pkl'.format(mode)\n",
    "    if os.path.isfile(save_path):\n",
    "        continue\n",
    "    features = data_process(path, mode)\n",
    "    save_features_to_pkl(features, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析训练数据中Vx和Vy的均值和标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205942, 50, 4)\n",
      "[ 0.01113458 -0.03050198] [0.67739089 0.81689616]\n"
     ]
    }
   ],
   "source": [
    "train_features = load_pkl_to_features(\"data/train.pkl\")\n",
    "print(train_features.shape)\n",
    "train_velocity_mean = train_features[:, 1:, 2:4].mean((0, 1))\n",
    "train_velocity_std = train_features[:, 1:, 2:4].std((0, 1))\n",
    "print(train_velocity_mean, train_velocity_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为Pytorch模型加载数据创建接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VelocityDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, pkl_filepath, mode, obs_len = 20, transform = None):\n",
    "        self.mode = mode\n",
    "        self.features = load_pkl_to_features(pkl_filepath)\n",
    "        self.obs_len = obs_len\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 返回source以及target velocity\n",
    "        src = self.features[idx, 1:self.obs_len, 2:4]\n",
    "        trg = self.features[idx, self.obs_len:, 2:4]\n",
    "        if self.transform:\n",
    "            src = self.transform(src)\n",
    "            trg = self.transform(trg)\n",
    "        return src, trg\n",
    "    \n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dataloader, func):\n",
    "        self.dataloader = dataloader\n",
    "        self.func = func\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        iter_dataloader = iter(self.dataloader)\n",
    "        for batch in iter_dataloader:\n",
    "            yield self.func(*batch)\n",
    "\n",
    "def batch_second(x, y):\n",
    "    \"\"\"\n",
    "    输入: x = [batch size, 19, 4], y = [batch size, 30, 4]\n",
    "    我们将batch size 放到第二个维度，从而使\n",
    "    x = [19, batch size, 4]\n",
    "    y = [30, batch size, 4]\n",
    "    \"\"\"\n",
    "    return x.transpose(0, 1), y.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理（转换为Tensor，归一化，batch换到第二个维度方便使用nn.Transformer）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    \"\"\"\n",
    "    对输入数据进行归一化处理\n",
    "    \"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = torch.Tensor(mean) \n",
    "        self.std = torch.Tensor(std)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = torch.Tensor(x)\n",
    "        return x.sub_(self.mean).div_(self.std)\n",
    "        \n",
    "transform = Normalize(mean=train_velocity_mean, std=train_velocity_std)\n",
    "\n",
    "train_dataset = VelocityDataset(\"data/train.pkl\", \"train\", transform=transform)\n",
    "val_dataset = VelocityDataset(\"data/val.pkl\", \"val\", transform=transform)\n",
    "test_dataset = VelocityDataset(\"data/test.pkl\", \"test\", transform=transform)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=6)\n",
    "train_loader = WrappedDataLoader(train_loader, batch_second)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=6)\n",
    "val_loader = WrappedDataLoader(val_loader, batch_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 128, 2]) torch.Size([30, 128, 2])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义Transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 source_seq_len: int = 19,\n",
    "                 target_seq_len: int = 30,\n",
    "                 input_dim: int = 2,\n",
    "                 output_dim: int = 2,\n",
    "                 d_model: int = 512,\n",
    "                 nhead: int = 8,\n",
    "                 num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6,\n",
    "                 dim_feedforward: int = 2048,\n",
    "                 dropout: float = 0.1,\n",
    "                 activation: str = 'relu'):\n",
    "        super().__init__()\n",
    "        self.source_seq_len = source_seq_len\n",
    "        self.target_seq_len = target_seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.total_seq_len = self.source_seq_len + self.target_seq_len\n",
    "        self.device = device\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "        \n",
    "        self.encoder_embedding = nn.Linear(input_dim, d_model)\n",
    "        self.decoder_embedding = nn.Linear(output_dim, d_model)\n",
    "        self.pos_embedding = nn.Embedding(self.total_seq_len, d_model)\n",
    "        self.transformer = nn.Transformer(d_model,\n",
    "                                          nhead,\n",
    "                                          num_encoder_layers,\n",
    "                                          num_decoder_layers,\n",
    "                                          dim_feedforward,\n",
    "                                          dropout,\n",
    "                                          activation)\n",
    "        self.linear = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        src shape: (19, batch size, 2)\n",
    "        trg shape: (30, batch size, 2)\n",
    "        \"\"\"\n",
    "        decoder_input_len = trg.shape[0]\n",
    "        decoder_mask = self.transformer.generate_square_subsequent_mask(decoder_input_len).to(self.device)\n",
    "        # decoder_mask shape: (30, 30), 用来进行30次计算（可并行），每次计算用mask的一行\n",
    "        \n",
    "        encoder_input = self.encoder_embedding(src) * self.scale\n",
    "        # encoder_input shape: (19, batch size, 512)\n",
    "        \n",
    "        encoder_pos_embedding =  self.pos_embedding(\n",
    "                                    torch.arange(0, self.source_seq_len, device=self.device)\n",
    "                                 ).unsqueeze(1)\n",
    "        # encoder_pos_embedding shape: (19, 1, 512), add second dim to allow broadcasting addition\n",
    "        \n",
    "        encoder_input += encoder_pos_embedding \n",
    "        \n",
    "        \n",
    "        # concat start of sequence (zeros) with trg without last item to form decoder input\n",
    "        start_of_sequence = torch.zeros(1, trg.shape[1], trg.shape[2], device=self.device)\n",
    "        decoder_input = torch.cat((start_of_sequence, trg[:-1, :, :]), 0)\n",
    "        \n",
    "        decoder_input = self.decoder_embedding(decoder_input) * self.scale + \\\n",
    "                        self.pos_embedding(\n",
    "                            torch.arange(self.source_seq_len,\n",
    "                                         self.total_seq_len,\n",
    "                                         device=self.device)\n",
    "                        ).unsqueeze(1)\n",
    "        # decoder_input shape: (30, batch size, 512)\n",
    "        \n",
    "        output = self.transformer(encoder_input, decoder_input, tgt_mask = decoder_mask)\n",
    "        # output shape: (30, batch size, 512)\n",
    "        \n",
    "        return self.linear(output)\n",
    "        # return tensor shape: (30, batch size, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型并初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "The model has 44,169,730 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrajectoryTransformer(\n",
       "  (encoder_embedding): Linear(in_features=2, out_features=512, bias=True)\n",
       "  (decoder_embedding): Linear(in_features=2, out_features=512, bias=True)\n",
       "  (pos_embedding): Embedding(49, 512)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    if hasattr(model, 'weight') and model.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(model.weight.data)\n",
    "        \n",
    "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(dev)\n",
    "model = TrajectoryTransformer(device = dev).to(dev)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建自定义损失函数EuclideanDistanceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3903)\n"
     ]
    }
   ],
   "source": [
    "def EuclideanDistanceLoss(output, target):\n",
    "    \"\"\"\n",
    "    output shape: (30, batch size, 2), 为预测的车辆速度\n",
    "    target shape is the same, 为车辆速度真值\n",
    "    这里我们计算所有预测速度和真实速度的欧几里得距离，然后将计算结果取平均得到我们的损失\n",
    "    \"\"\"\n",
    "    output = output.contiguous().view(-1, 2)\n",
    "    target = target.contiguous().view(-1, 2)\n",
    "    return F.pairwise_distance(output, target).mean()\n",
    "\n",
    "for x, y in train_loader:\n",
    "    print(EuclideanDistanceLoss(x, -x))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化optimizer以及criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "criterion = EuclideanDistanceLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义训练和测试流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg) in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        src = src.to(dev)\n",
    "        trg = trg.to(dev)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "            src = src.to(dev)\n",
    "            trg = trg.to(dev)\n",
    "            \n",
    "            output = model(src, trg)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义推理流程并做一些可视化\n",
    "推理是比较有难度的部分，看起来不是特别直观。\n",
    "\n",
    "注意尽管我们之前都是给模型直接传入理想的输出，但是在推理的时候，这个信息一般是没有的，所以我们需要迭代地计算出每一步的输出，并将这个输出加入到下一次的输入, 这个是大致的思路。\n",
    "\n",
    "另外，虽然我们在训练的时候传入的decode input的序列长度都是30，但是实际上Transformer decoder可以接受变长的序列长度输入，所以我们开始的时候可以直接给decoder传入一个序列长度的sequence，然后每一轮迭代将输出结果添加到这个sequence中，最后我们就获得了包含start of sequence以及30个预测点的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, agent_observed_vels):\n",
    "    \"\"\"\n",
    "    agent_observed_vels: [19, 2]\n",
    "    return predicted vels: [30, 2]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_input = torch.Tensor(agent_observed_vels).unsqueeze(1).to(model.device)\n",
    "        encoder_input = model.encoder_embedding(encoder_input) * model.scale + \\\n",
    "                        model.pos_embedding(\n",
    "                            torch.arange(0, model.source_seq_len, device=model.device)\n",
    "                        ).unsqueeze(1)\n",
    "        # encoder_pos_embedding shape: (19, 1, 512)\n",
    "        \n",
    "        # 保存encoder的输出结果, 之后传给decoder\n",
    "        memory = model.transformer.encoder(encoder_input)\n",
    "        \n",
    "        prediction = torch.zeros(model.target_seq_len + 1, 1, model.output_dim, device=model.device)\n",
    "        # prediction shape: (31, 1, 2), 用来保存start of sequence以及之后计算得到的输出序列 \n",
    "        for i in range(model.target_seq_len):\n",
    "            cur_len = i + 1\n",
    "            decoder_mask = model.transformer.generate_square_subsequent_mask(cur_len).to(model.device)\n",
    "            decoder_input = prediction[:cur_len]\n",
    "            decoder_input = model.decoder_embedding(decoder_input) * model.scale + \\\n",
    "                            model.pos_embedding(\n",
    "                                torch.arange(model.source_seq_len,\n",
    "                                             model.source_seq_len + cur_len,\n",
    "                                             device=model.device)\n",
    "                            ).unsqueeze(1)\n",
    "            out = model.transformer.decoder(decoder_input, memory, decoder_mask)\n",
    "            output = model.linear(out)\n",
    "            prediction[cur_len] = output[i]\n",
    "        return prediction[1:].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vels_to_coords(vels, start = np.zeros((1, 2))):\n",
    "    xy = np.zeros(vels.shape)\n",
    "    xy[0] = start\n",
    "    for i in range(1, vels.shape[0]):\n",
    "        xy[i] = vels[i]\n",
    "        xy[i] += xy[i - 1]\n",
    "    return xy\n",
    "        \n",
    "def plot_trajectory(prediction, groud_truth = None, obs_len = 20, start = np.zeros((1, 2))):\n",
    "    pred_xy = vels_to_coords(prediction, start)\n",
    "    figure = plt.figure(figsize=(10,10))\n",
    "    plt.scatter(pred_xy[:obs_len, 0], pred_xy[:obs_len, 1], label=\"observed trajectory\")\n",
    "    plt.scatter(pred_xy[obs_len:, 0], pred_xy[obs_len:, 1], label=\"prediction\")\n",
    "    if groud_truth is not None:\n",
    "        gt_xy = vels_to_coords(groud_truth, start)\n",
    "        plt.scatter(gt_xy[obs_len:, 0], gt_xy[obs_len:, 1], label=\"ground truth\")\n",
    "    plt.legend()\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_random_sample(model, dataset):\n",
    "    idx = random.randrange(len(dataset))\n",
    "    x, y = dataset[idx]\n",
    "    y_pred = predict(model, x)\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    x, y = x.cpu().numpy(), y.cpu().numpy()\n",
    "    zero = np.zeros((1, 2))\n",
    "    ground_truth = np.concatenate((zero, x, y), axis=0)\n",
    "    prediction = np.concatenate((zero, x, y_pred), axis=0)\n",
    "    return plot_trajectory(prediction, ground_truth)\n",
    "fig = plot_random_sample(model, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully load previous best model parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f651c807844a57ae854c176a419d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cb15f6964044ffbe19f839edbedd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1609.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_EPOCHES = 100\n",
    "writer = SummaryWriter(\"runs/transformer_experiment_3\")\n",
    "CLIP = 1\n",
    "\n",
    "# load previous best model params if exists\n",
    "model_dir = \"saved_models/ReformattedTransformer\"\n",
    "saved_model_path = model_dir + \"/best_transformer.pt\"\n",
    "if os.path.isfile(saved_model_path):\n",
    "    model.load_state_dict(torch.load(saved_model_path))\n",
    "    print(\"successfully load previous best model parameters\")\n",
    "    \n",
    "for epoch in tqdm(range(N_EPOCHES)):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    print(F'Epoch: {epoch+1:02}')\n",
    "    print(F'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(F'\\t Val. Loss: {val_loss:.3f}')\n",
    "    \n",
    "    step = start_epoch + epoch\n",
    "    writer.add_scalar('Loss/train', train_loss, step)\n",
    "    writer.add_scalar('Loss/val', val_loss, step)\n",
    "    writer.add_figure('Image/train', plot_random_sample(model, train_dataset), step)\n",
    "    writer.add_figure('Image/val', plot_random_sample(model, val_dataset), step)\n",
    "    writer.flush()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), saved_model_path)\n",
    "        \n",
    "start_epoch += N_EPOCHES\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
