{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle as pkl\n",
    "\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据获取和处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据目录下保存了很多csv文件，每个csv文件保存了一个5s的sequence，包含了多个车辆的轨迹信息\n",
    "DATASET_PATH = \"/ssd/datasets/argoverse/argoverse-forecasting-dataset/\"\n",
    "TRAIN = DATASET_PATH + \"train/data\"\n",
    "VAL = DATASET_PATH + \"val/data\"\n",
    "TEST = DATASET_PATH + \"test_obs/data/\"\n",
    "\n",
    "def data_process(root_dir, mode):\n",
    "    \"\"\"\n",
    "    处理root_dir目录下的CSV文件\n",
    "    返回numpy array, shape: (total num of sequences, seq len, 4)\n",
    "    \"\"\"\n",
    "    root_dir = pathlib.Path(root_dir)\n",
    "    paths = [(root_dir / filename).absolute() for filename in os.listdir(root_dir)]\n",
    "    seq_len = 50 if mode != \"test\" else 20\n",
    "    features = np.empty((len(paths), seq_len, 4))\n",
    "    for i in tqdm((range(len(paths)))):\n",
    "        path = paths[i]\n",
    "        sequence = pd.read_csv(path)\n",
    "        agent_x = sequence[sequence[\"OBJECT_TYPE\"] == \"AGENT\"][\"X\"]\n",
    "        agent_y = sequence[sequence[\"OBJECT_TYPE\"] == \"AGENT\"][\"Y\"]\n",
    "        xy = np.column_stack((agent_x, agent_y))\n",
    "        # 如果是train或者val，则xy shape: (50, 2) 记录了5秒（每秒10帧）的agent xy坐标\n",
    "        # 否则xy shape: (20, 2)，是agent前2s的xy坐标\n",
    "        vel = xy[1:] - xy[:-1]\n",
    "        init_unknown_vel = np.array([np.nan, np.nan])\n",
    "        vel = np.vstack((init_unknown_vel, vel))\n",
    "        # vel shape: (seq len, 2)，差分得到速度，初始速度无法获取，设为NaN\n",
    "        feature = np.column_stack((xy, vel))\n",
    "        # feature shape: (seq len, 4), 各列分别是x, y, vel_x, vel_y\n",
    "        features[i] = feature\n",
    "    return features\n",
    "\n",
    "def save_features_to_pkl(features, filepath):\n",
    "    basedir = os.path.dirname(filepath)\n",
    "    if not os.path.exists(basedir):\n",
    "        os.makedirs(basedir)\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pkl.dump(features, f)\n",
    "        \n",
    "def load_pkl_to_features(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        features = pkl.load(f)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c784a4312578417c90fcac811425e9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ef6fbe4e5f4a6c9cebaad74a6128cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=205942.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee4134e07df4e8e8799d5ce5335b182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=39472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b552fad8f024f13a4d6b0f8de1e0883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=78143.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = {\"train\": TRAIN,\n",
    "     \"val\": VAL,\n",
    "     \"test\": TEST} \n",
    "for mode, path in tqdm(d.items()):\n",
    "    features = data_process(path, mode)\n",
    "    save_path = 'data/{}.pkl'.format(mode)\n",
    "    save_features_to_pkl(features, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为Pytorch模型加载数据创建接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, pkl_filepath, mode, obs_len = 20):\n",
    "        self.mode = mode\n",
    "        self.features = load_pkl_to_features(pkl_filepath)\n",
    "        self.obs_len = obs_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 返回source以及target\n",
    "        return [self.features[idx, :self.obs_len], self.features[idx, self.obs_len:]]\n",
    "    \n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dataloader, func):\n",
    "        self.dataloader = dataloader\n",
    "        self.func = func\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        iter_dataloader = iter(self.dataloader)\n",
    "        for batch in iter_dataloader:\n",
    "            yield self.func(*batch)\n",
    "\n",
    "def batch_second(x, y):\n",
    "    \"\"\"\n",
    "    输入: x = [batch size, 20, 4], y = [batch size, 30, 4]\n",
    "    我们将batch size 放到第二个维度，从而使\n",
    "    x = [20, batch size, 4]\n",
    "    y = [30, batch size, 4]\n",
    "    \"\"\"\n",
    "    return x.transpose(0, 1), y.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrajectoryDataset(\"data/train.pkl\", \"train\")\n",
    "val_dataset = TrajectoryDataset(\"data/val.pkl\", \"val\")\n",
    "test_dataset = TrajectoryDataset(\"data/test.pkl\", \"test\")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=6)\n",
    "train_loader = WrappedDataLoader(train_loader, batch_second)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=6)\n",
    "val_loader = WrappedDataLoader(val_loader, batch_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 128, 4]) torch.Size([30, 128, 4])\n",
      "tensor([[ 7.3972e+02,  2.2534e+03,         nan,         nan],\n",
      "        [ 7.3963e+02,  2.2529e+03, -8.7988e-02, -5.7026e-01]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x[:2, 0, :])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义Transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 source_seq_len: int = 19,\n",
    "                 target_seq_len: int = 30,\n",
    "                 input_dim: int = 2,\n",
    "                 output_dim: int = 2,\n",
    "                 d_model: int = 512,\n",
    "                 nhead: int = 8,\n",
    "                 num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6,\n",
    "                 dim_feedforward: int = 2048,\n",
    "                 dropout: float = 0.1,\n",
    "                 activation: str = 'relu'):\n",
    "        super().__init__()\n",
    "        self.source_seq_len = source_seq_len\n",
    "        self.target_seq_len = target_seq_len\n",
    "        self.total_seq_len = self.source_seq_len + self.target_seq_len\n",
    "        self.device = device\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "        \n",
    "        self.encoder_embedding = nn.Linear(input_dim, d_model)\n",
    "        self.decoder_embedding = nn.Linear(output_dim, d_model)\n",
    "        self.pos_embedding = nn.Embedding(self.total_seq_len, d_model)\n",
    "        self.transformer = nn.Transformer(d_model,\n",
    "                                          nhead,\n",
    "                                          num_encoder_layers,\n",
    "                                          num_decoder_layers,\n",
    "                                          dim_feedforward,\n",
    "                                          dropout,\n",
    "                                          activation)\n",
    "        self.linear = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def generate_square_subsequent_mask(size):\n",
    "        \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \n",
    "        >>> generate_square_subsequent_mask(3)\n",
    "        tensor([[-inf, -inf, -inf],\n",
    "                [0., -inf, -inf],\n",
    "                [0., 0., -inf]])\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones(size, size))\n",
    "        mask = mask.float().masked_fill(mask == 1, float('-inf'))\n",
    "        return mask.to(self.device)\n",
    "        \n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        decoder_input_len = decoder_input.shape[0]\n",
    "        decoder_mask = self.generate_square_subsequent_mask(decoder_input_len)\n",
    "        # decoder_mask shape: (30, 30), 用来进行30次计算（可并行），每次计算用mask的一行\n",
    "        \n",
    "        encoder_input = self.encoder_embedding(encoder_input) * self.scale + \\\n",
    "                        self.pos_embedding(\n",
    "                            torch.arange(0, self.source_seq_len, device=self.device)\n",
    "                        ).unsqueeze(1)\n",
    "        # encoder_input shape: (19, batch size, 512)\n",
    "        \n",
    "        decoder_input = self.decoder_embedding(decoder_input) * self.scale + \\\n",
    "                        self.pos_embedding(\n",
    "                            torch.arange(self.source_seq_len,\n",
    "                                         self.total_seq_len,\n",
    "                                         device=self.device)\n",
    "                        ).unsqueeze(1)\n",
    "        # decoder_input shape: (30, batch size, 512)\n",
    "        \n",
    "        output = self.transformer(encoder_input, decoder_input, tgt_mask = decoder_mask)\n",
    "        # output shape: (30, batch size, 512)\n",
    "        \n",
    "        return self.linear(output)\n",
    "        # return tensor shape: (30, batch size, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型并初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 44,169,730 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrajectoryTransformer(\n",
       "  (encoder_embedding): Linear(in_features=2, out_features=512, bias=True)\n",
       "  (decoder_embedding): Linear(in_features=2, out_features=512, bias=True)\n",
       "  (pos_embedding): Embedding(49, 512)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    if hasattr(model, 'weight') and model.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(model.weight.data)\n",
    "        \n",
    "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TrajectoryTransformer(device = dev).to(dev)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
