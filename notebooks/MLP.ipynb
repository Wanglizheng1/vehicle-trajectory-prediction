{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import necessary libs/functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data import get_dataset # custom helper function to get dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the random seeds for deterministic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2333\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load train, val, test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = get_dataset([\"train\", \"val\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205942\n",
      "39472\n",
      "78143\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use single GPU or CPU depending on the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use dataloader to get batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=6)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205952\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader) * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 20, 2]) <class 'torch.Tensor'> torch.Size([16, 30, 2]) <class 'torch.Tensor'>\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "x, y = train_iter.next()\n",
    "print(x.size(), type(x), y.size(), type(y))\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(20 * 2, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 30 * 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # convert (batch_size, 20, 2) to (batch_size, 20 * 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=40, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=60, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "epoch 0, round 0/12872 train loss: 212.9060\n",
      "epoch 0, round 1000/12872 train loss: 209.8552\n",
      "epoch 0, round 2000/12872 train loss: 141.7605\n",
      "epoch 0, round 3000/12872 train loss: 298.7005\n",
      "epoch 0, round 4000/12872 train loss: 106.5699\n",
      "epoch 0, round 5000/12872 train loss: 169.1445\n",
      "epoch 0, round 6000/12872 train loss: 270.7253\n",
      "epoch 0, round 7000/12872 train loss: 88.2945\n",
      "epoch 0, round 8000/12872 train loss: 303.0291\n",
      "epoch 0, round 9000/12872 train loss: 174.8775\n",
      "epoch 0, round 10000/12872 train loss: 107.5720\n",
      "epoch 0, round 11000/12872 train loss: 126.4861\n",
      "epoch 0, round 12000/12872 train loss: 166.0462\n",
      "start validating...\n",
      "epoch 0, val loss: 2119.2402, time spend: 175.2303831577301s\n",
      "epoch 1, round 0/12872 train loss: 264.5938\n",
      "epoch 1, round 1000/12872 train loss: 144.3651\n",
      "epoch 1, round 2000/12872 train loss: 144.9422\n",
      "epoch 1, round 3000/12872 train loss: 117.1763\n",
      "epoch 1, round 4000/12872 train loss: 111.5341\n",
      "epoch 1, round 5000/12872 train loss: 123.6577\n",
      "epoch 1, round 6000/12872 train loss: 106.0782\n",
      "epoch 1, round 7000/12872 train loss: 61.9987\n",
      "epoch 1, round 8000/12872 train loss: 127.8399\n",
      "epoch 1, round 9000/12872 train loss: 180.3708\n",
      "epoch 1, round 10000/12872 train loss: 243.8767\n",
      "epoch 1, round 11000/12872 train loss: 67.1244\n",
      "epoch 1, round 12000/12872 train loss: 73.1391\n",
      "start validating...\n",
      "epoch 1, val loss: 1971.5587, time spend: 175.354248046875s\n",
      "epoch 2, round 0/12872 train loss: 91.2767\n",
      "epoch 2, round 1000/12872 train loss: 65.4954\n",
      "epoch 2, round 2000/12872 train loss: 251.2471\n",
      "epoch 2, round 3000/12872 train loss: 382.9996\n",
      "epoch 2, round 4000/12872 train loss: 100.2225\n",
      "epoch 2, round 5000/12872 train loss: 239.5178\n",
      "epoch 2, round 6000/12872 train loss: 160.1051\n",
      "epoch 2, round 7000/12872 train loss: 134.2605\n",
      "epoch 2, round 8000/12872 train loss: 163.4734\n",
      "epoch 2, round 9000/12872 train loss: 155.4662\n",
      "epoch 2, round 10000/12872 train loss: 61.7557\n",
      "epoch 2, round 11000/12872 train loss: 49.5515\n",
      "epoch 2, round 12000/12872 train loss: 138.2461\n",
      "start validating...\n",
      "epoch 2, val loss: 1844.5624, time spend: 175.01509737968445s\n",
      "epoch 3, round 0/12872 train loss: 203.1051\n",
      "epoch 3, round 1000/12872 train loss: 583.4095\n",
      "epoch 3, round 2000/12872 train loss: 136.6397\n",
      "epoch 3, round 3000/12872 train loss: 74.0193\n",
      "epoch 3, round 4000/12872 train loss: 166.4606\n",
      "epoch 3, round 5000/12872 train loss: 109.4103\n",
      "epoch 3, round 6000/12872 train loss: 835.3969\n",
      "epoch 3, round 7000/12872 train loss: 195.0158\n",
      "epoch 3, round 8000/12872 train loss: 107.6510\n",
      "epoch 3, round 9000/12872 train loss: 101.7219\n",
      "epoch 3, round 10000/12872 train loss: 150.4104\n",
      "epoch 3, round 11000/12872 train loss: 57.5994\n",
      "epoch 3, round 12000/12872 train loss: 272.1959\n",
      "start validating...\n",
      "epoch 3, val loss: 1642.8879, time spend: 174.60703301429749s\n",
      "epoch 4, round 0/12872 train loss: 87.1371\n",
      "epoch 4, round 1000/12872 train loss: 60.4313\n",
      "epoch 4, round 2000/12872 train loss: 234.9713\n",
      "epoch 4, round 3000/12872 train loss: 86.4819\n",
      "epoch 4, round 4000/12872 train loss: 51.8437\n",
      "epoch 4, round 5000/12872 train loss: 153.7131\n",
      "epoch 4, round 6000/12872 train loss: 119.0753\n",
      "epoch 4, round 7000/12872 train loss: 53.4130\n",
      "epoch 4, round 8000/12872 train loss: 46.1173\n",
      "epoch 4, round 9000/12872 train loss: 153.0815\n",
      "epoch 4, round 10000/12872 train loss: 65.1616\n",
      "epoch 4, round 11000/12872 train loss: 99.3907\n",
      "epoch 4, round 12000/12872 train loss: 73.7098\n",
      "start validating...\n",
      "epoch 4, val loss: 1501.6494, time spend: 173.42356729507446s\n",
      "epoch 5, round 0/12872 train loss: 89.9196\n",
      "epoch 5, round 1000/12872 train loss: 31.3707\n",
      "epoch 5, round 2000/12872 train loss: 211.1393\n",
      "epoch 5, round 3000/12872 train loss: 52.2797\n",
      "epoch 5, round 4000/12872 train loss: 48.4790\n",
      "epoch 5, round 5000/12872 train loss: 67.7571\n",
      "epoch 5, round 6000/12872 train loss: 125.9075\n",
      "epoch 5, round 7000/12872 train loss: 34.2632\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model.to(dev)\n",
    "epoches = 1000\n",
    "print(\"start training...\")\n",
    "for epoch in range(epoches):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for i, (xb, yb) in enumerate(train_loader):\n",
    "        xb = xb.to(dev)\n",
    "        yb = yb.to(dev).view(yb.size(0), -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        yb_pred = model(xb)\n",
    "        loss = loss_fn(yb_pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(\"epoch {}, round {}/{} train loss: {:.4f}\".format(epoch, i, len(train_loader), loss.item()))\n",
    "            \n",
    "    model.eval()\n",
    "    model_dir = \"../saved_model/MLP\"\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    torch.save(model.state_dict(), model_dir + \"/MLP_epoch{}\".format(epoch))\n",
    "    print(\"start validating...\")\n",
    "    with torch.no_grad():\n",
    "        val_loss = sum(loss_fn(model(xb.to(dev)), yb.to(dev).view(yb.size(0), -1)) for xb, yb in val_loader)\n",
    "    print(\"epoch {}, val loss: {:.4f}, time spend: {}s\".format(\n",
    "            epoch, val_loss / len(val_loader), time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
