{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from data import get_dataset # custom helper function to get dataset\n",
    "from models.Seq2Seq import Seq2Seq, Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dataloader, func):\n",
    "        self.dataloader = dataloader\n",
    "        self.func = func\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        iter_dataloader = iter(self.dataloader)\n",
    "        for batch in iter_dataloader:\n",
    "            yield self.func(*batch)\n",
    "            \n",
    "def preprocess(x, y):\n",
    "    # x and y is [batch size, seq len, feature size]\n",
    "    # to make them work with default assumption of LSTM,\n",
    "    # here we transpose the first and second dimension\n",
    "    # return size = [seq len, batch size, feature size]\n",
    "    return x.transpose(0, 1), y.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = get_dataset([\"train\", \"val\", \"test\"])\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=6)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=6)\n",
    "\n",
    "train_loader = WrappedDataLoader(train_loader, preprocess)\n",
    "val_loader = WrappedDataLoader(val_loader, preprocess)\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 256, 2])\n",
      "torch.Size([30, 256, 2])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 2\n",
    "OUTPUT_DIM = 2\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 4\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, dev).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (linear): Linear(in_features=2, out_features=128, bias=True)\n",
       "    (rnn): LSTM(128, 256, num_layers=4, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Linear(in_features=2, out_features=128, bias=True)\n",
       "    (rnn): LSTM(128, 256, num_layers=4, dropout=0.5)\n",
       "    (linear): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,949,826 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        # put data into GPU\n",
    "        x = x.to(dev)\n",
    "        y = y.to(dev)\n",
    "        \n",
    "        # zero all param gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # run seq2seq to get predictions\n",
    "        y_pred = model(x, y)\n",
    "        \n",
    "        # get loss and compute model trainable params gradients though backpropagation\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # update model params\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add batch loss, since loss is single item tensor\n",
    "        # we can get its value by loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            x = x.to(dev)\n",
    "            y = y.to(dev)\n",
    "            \n",
    "            # turn off teacher forcing\n",
    "            y_pred = model(x, y, teacher_forcing_ratio = 0)\n",
    "            \n",
    "            loss = criterion(y_pred, y)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully load previous best model parameters\n",
      "Epoch: 01 | Time: 2m 58s\n",
      "\tTrain Loss: 36629.582\n",
      "\t Val. Loss: 127474.822\n",
      "Epoch: 02 | Time: 2m 59s\n",
      "\tTrain Loss: 32897.483\n",
      "\t Val. Loss: 117809.916\n",
      "Epoch: 03 | Time: 2m 59s\n",
      "\tTrain Loss: 28319.573\n",
      "\t Val. Loss: 119895.755\n",
      "Epoch: 04 | Time: 2m 58s\n",
      "\tTrain Loss: 24012.915\n",
      "\t Val. Loss: 122357.672\n",
      "Epoch: 05 | Time: 2m 58s\n",
      "\tTrain Loss: 21408.488\n",
      "\t Val. Loss: 122434.448\n",
      "Epoch: 06 | Time: 2m 58s\n",
      "\tTrain Loss: 19969.975\n",
      "\t Val. Loss: 119592.575\n",
      "Epoch: 07 | Time: 2m 58s\n",
      "\tTrain Loss: 18388.910\n",
      "\t Val. Loss: 128100.033\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "N_EPOCHES = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# load previous best model params if exists\n",
    "model_dir = \"saved_models/Seq2Seq\"\n",
    "saved_model_path = model_dir + \"/best_seq2seq.pt\"\n",
    "if os.path.isfile(saved_model_path):\n",
    "    model.load_state_dict(torch.load(saved_model_path))\n",
    "    print(\"successfully load previous best model parameters\")\n",
    "    \n",
    "for epoch in range(N_EPOCHES):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    mins, secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(F'Epoch: {epoch+1:02} | Time: {mins}m {secs}s')\n",
    "    print(F'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(F'\\t Val. Loss: {val_loss:.3f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), saved_model_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
